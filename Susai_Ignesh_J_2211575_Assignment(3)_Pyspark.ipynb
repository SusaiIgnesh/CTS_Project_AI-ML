{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRcU55v94GOs"
   },
   "source": [
    "# **K-Means using Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxVknxZYytGK"
   },
   "source": [
    "### **Dataset - Breast Cancer Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGxECfZgy2AC"
   },
   "source": [
    "## **Installing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NytKsK6T3z8h",
    "outputId": "7aadc62e-c9d0-4e62-e481-1ed27e830a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 50.7 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=e644a2076c383a8bb1710e35b06481bcea8e55c6abdcbc67c90468e3d8b225eb\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldqJ-m-KPeh5"
   },
   "source": [
    "**PySpark** is the Python API(application programming interface)for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing.\n",
    "\n",
    "1.  **Apache Spark** is basically a computational engine that works with huge sets of data by processing them in parallel and batch systems. Spark is written in Scala, and PySpark was released to support the collaboration of Spark and Python. \n",
    "\n",
    "2.  you can install just a **PySpark package by using the pip python installer.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYcKxY88zAxh"
   },
   "source": [
    "**Installing PyDrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8LhEby74Q4p"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmGwLQRZPnEN"
   },
   "source": [
    "1. PyDrive is a high-level Python wrapper for the Google Drive API. It allows you to easily upload, download, and delete files in your Google Drive from a Python script. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50TeC0ikzCdE"
   },
   "source": [
    "**Installing Java Developer Kit 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4Nm3knL4WFk",
    "outputId": "b9e0ab0e-4114-446e-eb17-e7f4fb32fd24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  openjdk-8-jre-headless\n",
      "Suggested packages:\n",
      "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
      "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
      "  fonts-wqy-zenhei fonts-indic\n",
      "The following NEW packages will be installed:\n",
      "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
      "0 upgraded, 2 newly installed, 0 to remove and 22 not upgraded.\n",
      "Need to get 36.6 MB of archives.\n",
      "After this operation, 143 MB of additional disk space will be used.\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "(Reading database ... 123941 files and directories currently installed.)\n",
      "Preparing to unpack .../openjdk-8-jre-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../openjdk-8-jdk-headless_8u342-b07-0ubuntu1~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u342-b07-0ubuntu1~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
     ]
    }
   ],
   "source": [
    "!apt install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YDRvVEzPrs2"
   },
   "source": [
    "\n",
    "\n",
    "1. Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java.\n",
    "\n",
    "\n",
    "2. **OpenJDK (Open Java Development Kit)** is a free and open-source implementation of\n",
    "the Java Platform. it delivers the source code used to provide users with a wide choice of distributions on which to run their applications. \n",
    "\n",
    "\n",
    "3. **headless** are software  capable of working on a device without a graphical user interface. Such software receives inputs and provides output through other interfaces like network or serial port and is common on servers and embedded devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2GU6mgyzQ8d"
   },
   "source": [
    "**Importing OS and setting JAVA environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAof70Bn4p86"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3szsi-A0O9Pz"
   },
   "source": [
    " \n",
    "\n",
    "1. Python os system function allows us to run a command in the Python script.(**os — Miscellaneous operating system interfaces)**\n",
    "\n",
    "2. Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTPWKAY_zYiN"
   },
   "source": [
    "## **Importing PySpark, SparkContext & SparkConf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAd-qTRc5nbe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVcR0u6uP3Y4"
   },
   "source": [
    "\n",
    "\n",
    "1.   Import PySpark in Python Using findspark,path at runtime so that you can import PySpark modules.\n",
    "2.  **from pyspark.sql import *--->**PySpark SQL is a module in Spark which integrates relational processing with Spark's functional programming API. We can extract the data by using an SQL query language. We can use the queries same as the SQL\n",
    "\n",
    "3. **from pyspark.sql.types import *--->** It represents a list of available data types.\n",
    "\n",
    "4. **from pyspark.sql.functions import *-->** It represents a list of built-in functions available for DataFrame.\n",
    "\n",
    "5. **from pyspark import SparkContext, SparkConf --->** It is used to set a config option. Options set using this method are automatically propagated to both SparkConf and SparkSession's configuration.Returns the underlying SparkContext.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcjwJPc4zpzB"
   },
   "source": [
    "## **Creating Session & Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmdQQ9iD64PQ"
   },
   "outputs": [],
   "source": [
    "# create the session \n",
    "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "_pzaS0tE74hW",
    "outputId": "9394e286-fd17-45bb-b3a2-3ff0a118e2aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://45eedc7bff60:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb0088cf350>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TK3iLmFpV7BY"
   },
   "source": [
    "\n",
    "\n",
    "1. A **SparkSession** can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. To create a SparkSession.\n",
    "\n",
    "2. Now, we can import SparkSession from **pyspark.sql** and create a SparkSession, which is the entry point to Spark.\n",
    "\n",
    "3. You can give a name to the session using **appName()** and add some configurations with **config()** if you wish.\n",
    "\n",
    "4. **builder.getOrCreate()**\n",
    "Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder. This method first checks whether there is a valid global default SparkSession, and if yes, return that one. If no valid global default SparkSession exists, the method creates a new SparkSession and assigns the newly created SparkSession as the global default.\n",
    "\n",
    "5. Finally, print the SparkSession variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg5DlNQd0ARn"
   },
   "source": [
    "##**Downloading ngork**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXkA91IV9Nog",
    "outputId": "5674542a-dafc-4bcf-ef56-b4985fcc1f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-20 17:53:09--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.161.241.46, 54.237.133.81, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13832437 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.19M  40.1MB/s    in 0.3s    \n",
      "\n",
      "2022-10-20 17:53:10 (40.1 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13832437/13832437]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmM-ZxfY0FHy"
   },
   "source": [
    "1. wget is a convenient and widely supported tool for downloading files over three protocols: HTTP, HTTPS, and FTP. Wget owes its popularity to two of its main features: recursiveness and robustness. <br>\n",
    "Here we download ngork for linux 64 bit. ngork is the fastest way to put anything on the internet with a single command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwydhnYL0XVa"
   },
   "source": [
    "## **Unzipping ngork**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is_9nJERhmGx",
    "outputId": "1a3d82d7-9fb3-413e-d879-edd2319bbd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "get_ipython().system_raw('./ngrok http 4050 &')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wrqy1GGW0ZGK"
   },
   "source": [
    "1. In the previous cell, we have downloaded the ngork as zip format. Here, we use unzip command to extract the data from the zipped file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1I-8uig0jTN"
   },
   "source": [
    "## **Creating Spark UI and accessing it using through link**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OMGCCF_u1Pgf"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBYk3E5F1TPK"
   },
   "source": [
    "1. We have configured the Spark UI and started a Spark session in the previous cells. Here, this command is used to create a Colab application UI on the Apache Spark website under the Jobs tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-H5-_TOhmX-",
    "outputId": "893064ad-ab1c-42d5-a4ac-a1c97591851d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://123a-34-133-227-143.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSc56zi4bTR4"
   },
   "source": [
    "1. The above code creates and throws out a public URL for the UI page to view the Spark UI. Now we can view the jobs and their stages at the link created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o188Ahfx1-VH"
   },
   "source": [
    "## **Importing required Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_pxei_j90vi"
   },
   "source": [
    "**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8T7m94i90Fh"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jISFFZPpdhEP"
   },
   "source": [
    "1. We are importing dataset from sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "In5YWJpf-D3q",
    "outputId": "64404e98-d543-4061-eaf5-4551d4b6d0f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: double (nullable = false)\n",
      " |-- mean texture: double (nullable = false)\n",
      " |-- mean perimeter: double (nullable = false)\n",
      " |-- mean area: double (nullable = false)\n",
      " |-- mean smoothness: double (nullable = false)\n",
      " |-- mean compactness: double (nullable = false)\n",
      " |-- mean concavity: double (nullable = false)\n",
      " |-- mean concave points: double (nullable = false)\n",
      " |-- mean symmetry: double (nullable = false)\n",
      " |-- mean fractal dimension: double (nullable = false)\n",
      " |-- radius error: double (nullable = false)\n",
      " |-- texture error: double (nullable = false)\n",
      " |-- perimeter error: double (nullable = false)\n",
      " |-- area error: double (nullable = false)\n",
      " |-- smoothness error: double (nullable = false)\n",
      " |-- compactness error: double (nullable = false)\n",
      " |-- concavity error: double (nullable = false)\n",
      " |-- concave points error: double (nullable = false)\n",
      " |-- symmetry error: double (nullable = false)\n",
      " |-- fractal dimension error: double (nullable = false)\n",
      " |-- worst radius: double (nullable = false)\n",
      " |-- worst texture: double (nullable = false)\n",
      " |-- worst perimeter: double (nullable = false)\n",
      " |-- worst area: double (nullable = false)\n",
      " |-- worst smoothness: double (nullable = false)\n",
      " |-- worst compactness: double (nullable = false)\n",
      " |-- worst concavity: double (nullable = false)\n",
      " |-- worst concave points: double (nullable = false)\n",
      " |-- worst symmetry: double (nullable = false)\n",
      " |-- worst fractal dimension: double (nullable = false)\n",
      " |-- features: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "df = spark.createDataFrame(pd_df)\n",
    "\n",
    "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
    "    for struct_field in df.schema:\n",
    "        if struct_field.name in column_list:\n",
    "            struct_field.nullable = nullable\n",
    "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
    "    return df_mod\n",
    "df = set_df_columns_nullable(spark, df, df.columns)\n",
    "df = df.withColumn('features', array(df.columns))\n",
    "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIuax8AtjGw1"
   },
   "source": [
    "1. For convenience, given that the dataset is small, I will first construct a Pandas dataframe, tune the schema, and then convert it into a Spark datafram\n",
    "\n",
    "2. Converting the data to a pandas DataFrame\n",
    "\n",
    "3. Then converting this data frame to Spark Data Frame using createDataFrame method.\n",
    "\n",
    "4. Now data.schema contains three parts which are: **name,StringType,nullable (Which is set to true by default).**\n",
    "\n",
    "Now we create a function **'set_df_columns_nullable'** which converts the nullable into **False**, Thus, no** none values** are allowed in each of the columns. Next our function first converts the dataframe to a RDD (Using .rdd) and then returns a DataFrame  \n",
    "\n",
    "5. Now using the method .withcolumn we add another column (called features) to our data frame in which each row contains a list consisting of values of the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pQZxlVol_Ada"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "features = spark.createDataFrame(vectors.map(Row),[\"features\"])\n",
    "labels = pd.Series(breast_cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z80gSRkprmoz"
   },
   "source": [
    "1. Dense vectors are simply represented as NumPy array objects, so there is no need to covert them for use in MLlib. For sparse vectors, the factory methods in this class create an MLlib-compatible type, or users can pass in SciPy’s scipy.sparse column vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoQC35A6CMqC",
    "outputId": "ef992cb8-770e-49d9-c06e-a8cba98c370a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhoutte with squared eculidean distance = 0.8342904262826145\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# trains a K-means model\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(features)\n",
    "\n",
    "# Make Predictions\n",
    "prediction = model.transform(features)\n",
    "\n",
    "# Evaluate clustering by computing Silhoutte Score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhoutte = evaluator.evaluate(prediction)\n",
    "print('Silhoutte with squared eculidean distance = '+ str(silhoutte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYFKzZXmqxhZ"
   },
   "source": [
    "1. **from pyspark.ml.clustering import KMeans-->**Built on top of Spark, MLlib is a scalable machine learning library that provides a uniform set of high-level APIs that help users create and tune practical machine learning pipelines.\n",
    "\n",
    "2. **from pyspark.ml.evaluation import ClusteringEvaluator--->**Evaluator for Clustering results, which expects two input columns: prediction and features. The metric computes the Silhouette measure using the squared Euclidean distance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
